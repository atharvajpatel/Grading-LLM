[build-system]
requires = ["setuptools>=61.0", "wheel"]
build-backend = "setuptools.build_meta"

[project]
name = "grading_llm"
version = "0.1.0"
description = "Measure LLM consistency when grading statements across different granularities"
readme = "README.md"
requires-python = ">=3.10"
license = {text = "MIT"}
authors = [
    {name = "GRADING-LLM Team"}
]
keywords = ["llm", "grading", "pca", "interpretability", "openai", "judge"]
classifiers = [
    "Development Status :: 3 - Alpha",
    "Intended Audience :: Science/Research",
    "License :: OSI Approved :: MIT License",
    "Programming Language :: Python :: 3",
    "Programming Language :: Python :: 3.10",
    "Programming Language :: Python :: 3.11",
    "Programming Language :: Python :: 3.12",
]
dependencies = [
    "openai>=1.0.0",
    "numpy>=1.24.0",
    "scikit-learn>=1.3.0",
    "matplotlib>=3.7.0",
    "tenacity>=8.2.0",
    "python-dotenv>=1.0.0",
    "scipy>=1.10.0",
    "fastapi>=0.104.0",
    "uvicorn>=0.24.0",
    "tqdm>=4.66.0",
]

[project.optional-dependencies]
dev = [
    "pytest>=7.0.0",
    "pytest-asyncio>=0.21.0",
]

[project.scripts]
grading-single = "grading_llm.run_single:main"
grading-batch = "grading_llm.run_batch:main"
grading-prune = "grading_llm.prune_questions:main"
grading-server = "grading_llm.api:run_server"

[tool.setuptools.packages.find]
where = ["src"]

[tool.pytest.ini_options]
asyncio_mode = "auto"
testpaths = ["tests"]
